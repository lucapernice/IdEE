---
layout: post
title: "Integration of Genetic Algorithms and Language Models: Part 1"
author: luca
categories: [ LLM, Artificial Intelligence ]
image: assets/images/dna.jpeg
description: "This article explores how GAs evolve effective prompts for LLMs, empowering the language model to generate high-quality outputs"
featured: true
---

When utilizing our **large language model** for specific tasks, its performance isn’t solely dependent on the model itself but also on the provided **prompt**. Consequently, multiple attempts might be necessary to discover a prompt yielding a satisfactory solution. Alternatively, implementing a **genetic algorithm** could facilitate this process by evolving initial prompts. Through automated iterations, this algorithm can eventually yield the most effective prompt discovered. 

The first segment of this report focuses on a **fundamental implementation** of this concept. The latter part delves into a more **advanced approach** wherein the language model is leveraged to evolve Python code for the creation of new **neural network architectures**.

### A (very) short introduction to Genetic Algorithm

**Genetic algorithms** (GAs) are optimization techniques inspired by the principles of **Darwinian evolution**. These algorithms mimic the process of natural selection where the fittest individuals are selected for reproduction to produce the offspring of the next generation.

#### Key Concepts

1. **Population**: A set of potential solutions to the optimization problem.
2. **Chromosomes**: Representations of solutions, typically as strings or arrays.
3. **Fitness Function**: Evaluates how well each solution performs the task.
4. **Selection**: The process of choosing the best solutions for reproduction.
5. **Crossover**: Combines parts of two solutions to create a new solution.
6. **Mutation**: Introduces small random changes to solutions to maintain diversity.

#### Process

1. **Initialization**: Start with a randomly generated population of solutions.
2. **Evaluation**: Use the fitness function to assess the performance of each solution.
3. **Selection**: Select the top-performing solutions for reproduction.
4. **Crossover and Mutation**: Apply crossover and mutation to create a new population.
5. **Iteration**: Repeat the evaluation, selection, and reproduction steps until an optimal solution is found or a set number of generations is reached.

#### Effectiveness

Genetic algorithms are effective because they explore a wide range of solutions and can adapt to complex, multidimensional landscapes. They are particularly useful when the search space is large and traditional optimization methods are impractical. By simulating natural evolution, GAs can find high-quality solutions for a variety of optimization problems in fields such as engineering, economics, and artificial intelligence.

### 1. EvoPrompt

**Reference Paper:** [Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers](https://arxiv.org/abs/2309.08532)

**Colab Notebook:** [https://colab.research.google.com/drive/1kWnaktaJ0KvtNjuH8dUkaZkbXOAXDDYV?usp=sharing](https://colab.research.google.com/drive/1kWnaktaJ0KvtNjuH8dUkaZkbXOAXDDYV?usp=sharing)

When working with a **large language model** for specific tasks, fine-tuning the model becomes imperative for improved results. However, this process can be notably costly. An alternative approach involves retaining the parameters of **Large Language Models (LLMs)** while employing continuous prompt tuning methods.

Yet, in certain cases—such as when utilizing **GPT-4**—accessing model parameters is impossible. Considering this limitation, employing **Genetic Algorithms (GAs)** to evolve discrete prompts emerges as a viable solution. This approach avoids involvement with the model’s parameters, focusing solely on crafting sentences for prompts. Despite its apparent suitability, evolving natural language expressions presents challenges.

For instance, ensuring **human-comprehensibility** and **coherence** after a random crossover between two expressions is not straightforward. **Evolutionary Algorithms (EAs)** operators commonly struggle to grasp token relations as they are predominantly designed for sequences. However, the solution lies within the **LLMs** themselves, as they are specifically engineered to comprehend and generate natural language. The proposed strategy involves a synergy between LLMs and EAs, where **language models generate new prompts and execute crossover and mutation operations**, while EAs guide the optimization process. This collaboration aims to harness the strengths of both methodologies for enhanced efficiency in prompt evolution.

To conduct all the computations required for the evolutionary cycle without the need for a powerful GPU, the code is executed in a **Colab Notebook**, leveraging the **Stable Beluga 2 model** hosted by **Petals** ([https://github.com/bigscience-workshop/petals](https://github.com/bigscience-workshop/petals)), a community-based system that facilitates GPU sharing. For more information, refer to the provided links.

### Exploring the Colab Code

Let’s delve into the Colab code. In the initial portion of the notebook, the **Stable Beluga 2 model** is employed to perform **crossover** on two sentences. The first sentence is "Joe wants to eat a pizza." while the second is "Mark loves to play football." Performing the crossover necessitates crafting an appropriate input for the model. This presents a challenge that varies depending on the model’s configuration:

- Some models generate completions for given prompt sentences.
- Others are designed to receive instructions as prompts and execute the requested task.
- Some models are specialized for engaging in conversations (chatbots).

While not essential for this specific test (it will be addressed in the implementation section), an effective prompt should enable the model to generate offspring in a format that facilitates the automation of the evolutionary process. The model’s output, the offspring, then becomes the input for the model to be evaluated. 

Remember, the primary objective is to evolve prompts, and prompts should be composed of meaningful strings of text.

The initial prompt tested is: 

```
Prompt 1: Joe wants to eat a pizza. 
Prompt 2: Mark loves to play football. 
Crossover:
```

The model’s completion is:

```
1. Joe wants to eat a pizza while watching Mark play football. 
2. Mark loves to play football with Joe while they eat pizza together.
Crossover Prompt
```

The format of the output string is not satisfactory, and the offspring are significantly longer than the input prompts. To address these issues, the model is provided with an example of how to perform crossover (**One Shot Prompting**). This approach can be extended to multiple examples (**Few Shot Prompting**), which can facilitate the model’s ability to learn new tasks without requiring additional training data. 

For instance, the revised prompt is:

```
Prompt 1: Lisa is having fun. 
Prompt 2: Jean is bored. 
Crossover: 
1. Lisa is bored. 
2. Jean is having fun.
Prompt 1: Joe wants to eat a pizza. 
Prompt 2: Mark loves to play football.
Crossover:
```

While the output is not explicitly provided here (it can be found in the Colab notebook), the two offsprings can be readily identified as:

```
Joe wants to play football.
Mark wants to eat a pizza.
```

Even though the offsprings now have the same length as the input sentences, the example in the prompt introduces a significant bias into the crossover operation.

Substituting certain words with others can significantly impact the output, which is why **prompt evolution** is employed. By incorporating the term "offsprings" into the crossover prompt, shorter sentences are produced without the need for any examples. The Colab notebook includes numerous additional tests involving the **Stable Beluga 2 model** performing both crossover and mutation operations. These tests demonstrate the model’s ability to perform these operations, albeit with some refinements required before automation.

For these improvements, the paper *“Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers”* serves as a guideline.

**Figure 1** displays a crafted prompt intended for executing both crossover and mutation. This prompt directs the language model to "follow the instructions step-by-step" to produce the final prompt within brackets, achieving the desired outcome. Any algorithm that **LLMs** can execute by providing detailed step-by-step instructions can be considered.

<div style="text-align:center">
  <figure>
    <img src="{{ site.baseurl }}/assets/images/Schermata del 2023-12-17 12-36-20.png" alt="Figure 1" width="70%">
    <figcaption>Figure 1: Image from "Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers" paper </figcaption>
  </figure>
</div>

The identical prompt proposed in the paper functions well with **Stable Beluga 2**, enabling us to proceed with a functional **GA implementation**. It’s important to note that this project aims to demonstrate a functional implementation within the constraints of limited hardware resources and time availability. Computational work involves not just generating tokens with LLMs but also evaluating them.

While running the Stable Beluga 2 model isn’t an issue thanks to **Petals**, the focus remains on minimizing the evaluation component. The simple evaluation suggested in the Colab notebook involves tallying the occurrences of 'a' and 'A' characters in the output string derived from the provided prompt. Although this might seem like a less sophisticated metric, it’s incredibly easy to implement and lightweight in computational terms, particularly as only a fixed maximum number of tokens are generated.


The current objective is to demonstrate that the **Genetic Algorithm (GA)**, executed by **Stable Beluga 2**, can guide the optimization process from an initial population towards retaining an optimal prompt. The aim is to find a prompt that generates an output string with a high count of 'a' and 'A' characters.

#### Initial Population

The initial population consists of 10 randomly generated questions created by **Google Bard**. While not listing all of them here, examples of these questions include:

- "In the symphony of life, what instrument plays the melody of resilience?"
- "In the relentless pursuit of dreams, what ingredient holds the key to unwavering determination?"

### Evaluation and Evolution Process

Subsequently, each prompt undergoes the previously discussed evaluation process: the prompt generates an answer string, and a function tallies the occurrences of 'a' and 'A' characters within the string.

#### Selection Phase

The selection phase in the Colab implementation diverges from the approach outlined in the paper. Specifically, a **tournament selection** method is employed. During the evolution phase, each individual generates offspring by pairing with another random individual from the population. In contrast, the paper utilizes a **roulette wheel selection** method to choose the two parents before executing crossover. There are various potential implementations for this aspect, considering the diverse settings within genetic algorithms, and determining the most effective methods can be a challenging task.

#### Evolution Process

Finally, the evolution can be performed. Given that the program operates on a Colab virtual machine, both the population and fitness for each iteration are stored in an external file. This file can be downloaded to a local machine, ensuring persistence. Consequently, the evolution process can be interrupted at any point and resumed from the last iteration on another runtime. The existence of a file containing the evolution’s data also enables a comprehensive review of the entire history and facilitates the creation of a graph (Figure 2).


<div style="text-align:center">
  <figure>
    <img src="{{ site.baseurl }}/assets/images/download (4).png" alt="Figure 2" width="70%">
    <figcaption>Figure 2: Fitness evaluation history</figcaption>
  </figure>
</div>


### Overcoming Local Optima

After 10 iterations, it appears that the search has become trapped in a local optimum. This is evidenced by the final populations comprising repeated individuals. This situation arises because the model is performing crossovers with identical sentences, relying solely on mutations for variation.

#### Adjusting Temperature and Selection Process

To address this, adjusting the temperature of the **Stable Beluga 2** generating function allows for the generation of more diverse outputs and consequently, diverse mutations. Employing a higher temperature, possibly an adaptive one, along with a selection process exerting less pressure, could aid in escaping such local optima within a reasonable number of iterations. 

Despite this setback, the fitness has shown significant improvement within a few iterations, underscoring the algorithm’s potential effectiveness.

#### Exploring Differential Evolution

Previously, it was mentioned that one could attempt any algorithm using **LLMs** by crafting effective step-by-step instruction prompts. Indeed, the paper also details how to implement **Differential Evolution (DE)**. The creators of EvoPrompt demonstrated that, at times, the DE variant yields superior results compared to the GA variant.

However, in the final segment of the Colab notebook, **Stable Beluga 2** fails to generate the expected output within brackets, as it did previously. According to the paper, the authors executed the evolutionary operators using **GPT-3.5**, indicating that they designed the prompt specifically for that model. Nevertheless, it’s uncertain whether this prompt would function effectively in other language models.


